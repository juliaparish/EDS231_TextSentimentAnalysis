---
title: "Topic 04 - Sentiment Analysis II"
author: "Julia Parish"
date: '2022-04-26'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Sentiment Analyis II

This text sentiment analysis was completed as an assignment for the course, Environmental Data Science 231: Text and Sentiment Analysis for Environmental Problems. The data was sourced from Twitter. 

Original assignment instructions can be found [here](https://maro406.github.io/EDS_231-text-sentiment/topic_4.html)

### Load Libraries
```{r}
library(quanteda)
library(quanteda.sentiment)
library(quanteda.textstats)
library(tidyverse)
library(tidytext)
library(lubridate)
library(wordcloud)
library(reshape2)
library(here)
library(rtweet)
library(paletteer)

```

## Load IPPC tweet data & create plot of data
```{r ipcc tweet data}
raw_tweets <- read.csv("https://raw.githubusercontent.com/MaRo406/EDS_231-text-sentiment/main/dat/IPCC_tweets_April1-10_sample.csv", header=TRUE)

dat<- raw_tweets[,c(5,7)] # Extract Date and Title fields

tweets <- tibble(text = dat$Title,
                  id = seq(1:length(dat$Title)),
                 date = as.Date(dat$Date,'%m/%d/%y'))

head(tweets$text, n = 10)
```

```{r}
#simple plot of tweets per day
tweets %>%
  count(date) %>%
  ggplot(aes(x = date, y = n))+
  geom_line() +
  labs(title = "IPCC Tweets per Day",
       subtitle = "April 01 - April 10, 2022",
       x = "Date",
       y = "Number of Tweets") +
  theme_minimal()
```

# Questions

## 1. Think about how to further clean a twitter data set. Let’s assume that the mentions of twitter accounts is not useful to us. Remove them from the text field of the tweets tibble.

```{r}

# keep original text column to track changes
tweets_clean <- tweets %>% 
  mutate(text_clean = text)  

# remove mentions and website links
tweets_clean$text_clean <- str_remove(tweets_clean$text_clean, "@[a-z,A-Z]*")

tweets_clean$text_clean <- str_remove(tweets_clean$text_clean, "[:digit:]")

tweets_clean$text_clean <- gsub("http.*","", tweets_clean$text_clean)

tweets_clean$text_clean <- gsub("https.*","", tweets_clean$text_clean)

# remove punctuations
tweets_clean$text_clean <- gsub('[[:punct:]]', '', tweets_clean$text_clean) 

#tokenise tweets and remove stop words
words <- tweets_clean %>%
  select(id, date, text, text_clean) %>%
  unnest_tokens(output = word, input = text_clean, token = "words") %>%
  anti_join(stop_words, by = "word")

#clean tokens
# remove numbers
clean_tokens <- str_remove_all(words$word, "[:digit:]")

# remove mentions
clean_tokens <- str_remove_all(clean_tokens, "@[a-z,A-Z]*")

# remove apostrophes
clean_tokens <- gsub("’s", '', clean_tokens)

# remove unnecessary twitter formats
clean_tokens <- str_remove_all(clean_tokens, "t.co")

# stem the token "ipcc" as there are some plural instances
clean_tokens <- str_replace_all(string = clean_tokens,
                                pattern = "ipcc[a-z, A-Z]*", 
                                replacement = "ipcc")


# stem the token "fuel" as it may occur in the plural form
clean_tokens <- str_replace_all(string = clean_tokens,
                                pattern = "fuel[a-z, A-Z]*", 
                                replacement = "fuel")

# stem the token "biofuel" as it may occur in the plural form
clean_tokens <- str_replace_all(string = clean_tokens,
                                pattern = "biofuel[a-z, A-Z]*", 
                                replacement = "biofuel")

# stem the token "headline" as it may occur in the plural form
clean_tokens <- str_replace_all(string = clean_tokens,
                                pattern = "headline[a-z, A-Z]*", 
                                replacement = "headline")

# stem the token "regulation" as it may occur in the plural form
clean_tokens <- str_replace_all(string = clean_tokens,
                                pattern = "regulation[a-z, A-Z]*", 
                                replacement = "regulation")

# stem the token "follower" as it may occur in the plural form
clean_tokens <- str_replace_all(string = clean_tokens,
                                pattern = "follower[a-z, A-Z]*", 
                                replacement = "follower")

# stem the token "suggestion" as it may occur in the plural form
clean_tokens <- str_replace_all(string = clean_tokens,
                                pattern = "suggestion[a-z, A-Z]*", 
                                replacement = "suggestion")

# stem the token "solution" as it may occur in the plural form
clean_tokens <- str_replace_all(string = clean_tokens,
                                pattern = "solution[a-z, A-Z]*", 
                                replacement = "solution")

# stem the token "reduction" as it may occur in the plural form
clean_tokens <- str_replace_all(string = clean_tokens,
                                pattern = "reduction[a-z, A-Z]*", 
                                replacement = "reduction")

# stem the token "risk" as it may occur in the plural form
clean_tokens <- str_replace_all(string = clean_tokens,
                                pattern = "risk[a-z, A-Z]*", 
                                replacement = "risk")

# stem the token "scenario" as it may occur in the plural form
clean_tokens <- str_replace_all(string = clean_tokens,
                                pattern = "scenario[a-z, A-Z]*", 
                                replacement = "scenario")

# stem the token "submission" as it may occur in the plural form
clean_tokens <- str_replace_all(string = clean_tokens,
                                pattern = "submission[a-z, A-Z]*", 
                                replacement = "submission")

words$clean <- clean_tokens

# remove the empty strings
tib <-subset(words, clean != "")

#reassign
words <- tib

head(words)
```

## 2. Compare the ten most common terms in the tweets per day. Do you notice anything interesting?

```{r}
words_freq <- words %>% 
  group_by(clean) %>% 
  summarise(n()) %>% 
  top_n(10) %>% 
  rename("freq" = "n()") %>% 
  select(clean)

words_top10 <- inner_join(words_freq, words, by = "clean") %>% 
  group_by(date, clean) %>% 
  summarize(n()) %>% 
  rename("freq" = "n()")
```

```{r}
top10term_plot <- ggplot(data = words_top10, aes(x = date, y = freq)) +
  geom_line(aes(color = clean)) +
  labs(title = "10 Most Common IPCC-related Tweet Terms",
       subtitle = "April 01 - April 10, 2022",
       x = "Date",
       y = "Frequency",
       color = "Term") +
  scale_color_paletteer_d("rcartocolor::Safe") +
  theme_minimal() 

top10term_plot
```


## 3. Adjust the wordcloud in the “wordcloud” chunk by coloring the positive and negative words so they are identifiable.

```{r}
#load sentiment lexicons
bing_sent <- get_sentiments('bing')
nrc_sent <- get_sentiments('nrc')
```

```{r}
cloud <- words %>% inner_join(get_sentiments("bing")) %>%
  inner_join(get_sentiments("nrc")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("slateblue3", "goldenrod2"),
                   max.words = 100)

cloud
```

## 4. Let’s say we are interested in the most prominent entities in the Twitter discussion. Which are the top 10 most tagged accounts in the data set. Hint: the “explore_hashtags” chunk is a good starting point.

```{r create_corpus}
corpus <- corpus(dat$Title) #enter quanteda
#summary(corpus)
# text: tweet ID, Types: species words, Tokens: total words
```

```{r user accounts}
tagged_accts <- tokens(corpus, remove_punct = TRUE) %>% 
               tokens_keep(pattern = "@*")

# feature matrix - shows location of each features in the corpus aka located in the tweet : document feature matrix
dfm_tags<- dfm(tagged_accts)

# frequency of hashtags
tstat_freq <- textstat_frequency(dfm_tags, n = 100)
head(tstat_freq, 10)

#tidytext gives us tools to convert to tidy from non-tidy formats
tags_tib <- tidy(dfm_tags)

tags_tib %>%
   count(term) %>%
   with(wordcloud(term, n, color = "slateblue3", max.words = 100))
```

```{r}
top_tags <- tags_tib %>% 
  group_by(term) %>% 
  summarize(n()) %>% 
  rename("freq" = "n()") %>% 
  top_n(10) 

```

```{r, fig.cap="Top 10 Twitter Accounts tagged in IPCC related tweets between April 1 - April 10, 2022"}
top10user_plot <- top_tags %>% 
  mutate(term = fct_relevel(term, 
            "@ipcc_ch", "@logicalindians", "@antonioguterres", "@nytimes", "@yahoo", "@potus", "@un", "@youtube", "@conversationedu", "@ipcc")) %>%
  ggplot(aes(x = freq, y = term)) +
  geom_point(color = "slateblue2") +
  labs(title = "Top 10 Tagged IPCC-related Accounts",
       subtitle = "April 01 - April 10, 2022",
       x = "Frequency of Mentions",
       y = "Twitter Account",
       caption = "Data Source: Twitter") +
  theme_minimal() 

top10user_plot
```


## 5. The Twitter data download comes with a variable called “Sentiment” that must be calculated by Brandwatch. Use your own method to assign each tweet a polarity score (Positive, Negative, Neutral) and compare your classification to Brandwatch’s (hint: you’ll need to revisit the “raw_tweets” data frame).

```{r}
# Extract Date, Title, and Sentiment fields
dat2<- raw_tweets[,c(5, 7, 11)] 

tweet_sen <- tibble(element_id = seq(1:length(dat2$Title)),
                    date = as.Date(dat2$Date,'%m/%d/%y'),
                    text = dat2$Title,
                    brandwatch_sentiment = dat2$Sentiment)
```


## Bonus - Emoji Frequency Exploration

```{r}
# extract emojis from tweets 
all_emojis_in_tweets <- emojis %>% 
  # for each emoji, find tweets containing this emoji       
  mutate(tweet = map(code, ~grep(.x, tweets_clean$text))) %>% 
  unnest(tweet) %>%
  # count the number of tweets in which each emoji was found           
  count(code, description) %>% 
  mutate(emoji = paste(code, description)) 
```

```{r}
all_emojis_in_tweets %>% 
  top_n(5, n) %>% 
ggplot() +
  geom_col(aes(x = fct_reorder(emoji, n), y = n, fill = n), 
           color = "grey58", width = 1) +
  scale_fill_gradientn("n", colors = brewer.pal(5, "Set2")) +
  labs(x = "", y = "Count", 
       title = "Most used emojis in IPCC tweets",
       subtitle = "April 01 - April 10, 2022") +
  coord_flip()

```


