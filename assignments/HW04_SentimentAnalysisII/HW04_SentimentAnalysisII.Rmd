---
title: "Topic 04 - Sentiment Analysis II"
author: "Julia Parish"
date: '2022-04-26'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Sentiment Analyis II

This text sentiment analysis was completed as an assignment for the course, Environmental Data Science 231: Text and Sentiment Analysis for Environmental Problems. The data was sourced from Twitter. 

Original assignment instructions can be found [here](https://maro406.github.io/EDS_231-text-sentiment/topic_4.html)

### Load Libraries
```{r}
library(quanteda)
library(quanteda.sentiment)
library(quanteda.textstats)
library(tidyverse)
library(tidytext)
library(lubridate)
library(wordcloud)
library(reshape2)
library(here)
library(rtweet)
library(paletteer)

```

## Load IPPC tweet data & create plot of data
```{r ipcc tweet data}
raw_tweets <- read.csv("https://raw.githubusercontent.com/MaRo406/EDS_231-text-sentiment/main/dat/IPCC_tweets_April1-10_sample.csv", header=TRUE)

dat<- raw_tweets[,c(5,7)] # Extract Date and Title fields

tweets <- tibble(text = dat$Title,
                  id = seq(1:length(dat$Title)),
                 date = as.Date(dat$Date,'%m/%d/%y'))

head(tweets$text, n = 10)
```

```{r}
#simple plot of tweets per day
tweets %>%
  count(date) %>%
  ggplot(aes(x = date, y = n))+
  geom_line() +
  labs(title = "IPCC Tweets per Day",
       subtitle = "April 01 - April 10, 2022",
       x = "Date",
       y = "Number of Tweets") +
  theme_minimal()
```

# Questions

## 1. Think about how to further clean a twitter data set. Let’s assume that the mentions of twitter accounts is not useful to us. Remove them from the text field of the tweets tibble.

```{r}

# keep original text column to track changes
tweets_clean <- tweets %>% 
  mutate(text_clean = text)  

# remove mentions and website links
tweets_clean$text_clean <- str_remove(tweets_clean$text_clean, "@[a-z,A-Z]*")
tweets_clean$text_clean <- str_remove(tweets_clean$text_clean, "[:digit:]")

tweets_clean$text_clean <- gsub("http.*","", tweets_clean$text_clean)

tweets_clean$text_clean <- gsub("https.*","", tweets_clean$text_clean)

# remove punctuations
tweets_clean$text_clean <- gsub('[[:punct:]]', '', tweets_clean$text_clean) 

#tokenise tweets and remove stop words
words <- tweets_clean %>%
  select(id, date, text, text_clean) %>%
  unnest_tokens(output = word, input = text_clean, token = "words") %>%
  anti_join(stop_words, by = "word")

#clean tokens
# remove numbers
clean_tokens <- str_remove_all(words$word, "[:digit:]")

# remove mentions
clean_tokens <- str_remove_all(clean_tokens, "@[a-z,A-Z]*")

# remove apostrophes
clean_tokens <- gsub("’s", '', clean_tokens)

# remove unnecessary twitter formats
clean_tokens <- str_remove_all(clean_tokens, "t.co")

words$clean <- clean_tokens

# remove the empty strings
tib <-subset(words, clean != "")

#reassign
words <- tib

```

```{r}
ipcc_tweets_clean <- tweets_clean %>%
  dplyr::select(text_clean) %>%
  unnest_tokens(word, text_clean) %>%
  anti_join(stop_words, by = "word")
```


## 2. Compare the ten most common terms in the tweets per day. Do you notice anything interesting?

## 3. Adjust the wordcloud in the “wordcloud” chunk by coloring the positive and negative words so they are identifiable.

## 4. Let’s say we are interested in the most prominent entities in the Twitter discussion. Which are the top 10 most tagged accounts in the data set. Hint: the “explore_hashtags” chunk is a good starting point.

## 5. The Twitter data download comes with a variable called “Sentiment” that must be calculated by Brandwatch. Use your own method to assign each tweet a polarity score (Positive, Negative, Neutral) and compare your classification to Brandwatch’s (hint: you’ll need to revisit the “raw_tweets” data frame).


## Bonus - Emoji Frequency Exploration

```{r}
# extract emojis from tweets 
all_emojis_in_tweets <- emojis %>% 
  # for each emoji, find tweets containing this emoji       
  mutate(tweet = map(code, ~grep(.x, tweets_clean$text))) %>% 
  unnest(tweet) %>%
  # count the number of tweets in which each emoji was found           
  count(code, description) %>% 
  mutate(emoji = paste(code, description)) 
```

```{r}
all_emojis_in_tweets %>% 
  top_n(5, n) %>% 
ggplot() +
  geom_col(aes(x = fct_reorder(emoji, n), y = n, fill = n), 
           color = "grey58", width = 1) +
  scale_fill_gradientn("n", colors = brewer.pal(5, "Set2")) +
  labs(x = "", y = "Count", 
       title = "Most used emojis in IPCC tweets",
       subtitle = "April 01 - April 10, 2022") +
  coord_flip()

```


